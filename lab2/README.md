# Implementation:
## Representation of the problem
To represent the problem we use two numpy arrays:
* elementCovers: contains N lists, one for each number in [0, N-1]. Each list i contains the indexes of the original lists generated by problem() that contains the number i. That way, given a number i, it can directly access the lists that covers it.
* listsCost: for each list generated by problem() it keep track of the total number of elements.

## Representation of individuals:
An individual genome is given by a list of N elements. Each element i represent the index of a list containing i. That way, each individual is always a feasible solution. Lists can appear many times in the same genome.

The fitness if given by the number of elements of the lists included in its genome (duplicates not counted). This isn't actually a fitness function but a cost function; the program simply favour individual with lower values.

Random mutations are given by:
* Chose one gene i
* Change it with a random list that covers number i.

Offsprings are generated by:
* For each element i in [0, N-1]: gene i is chosen between one of the two parent's gene i.

Execution of the program is:
* Generate a random population
* While (Generations < GENERATIONS):
* * Sort population by fitness and keep the best TOURNAMENT_SIZE individuals. Discard the others.
* * Keep track of the best individual so far.
* * Generate OFFSPRINGS new individual by chosing 2 random ones from population. (population has already been reduced)
* * Mutate randomly the new offsprings
* * New population is given by the offsprings, repeat the loop.

# Result:
## Configuration:
* POPULATION_SIZE = 2000
* TOURNAMENT_SIZE = 500
* OFFSPRINGS = 4000
* GENERATIONS = 80
* MUTATIONS = 2
# # Solutions:
* N=5: 5 elements. **Best solution**
* N=10: 10 elements. **Best solution**
* N=20: 23 elements. **Best solution**
* N=50: 103  elements. **Not the best solution**
* N=100: 358 elements. **Unknown if best solution - most likely not**

# Different configurations:
By playing around with configurations I found out:
* Mutations most of times generates worst results. Perhaps if the population size and the number of generations were significantly bigger it wouldn't be that way; but with this numbers it seems like most of times they create worst individuals for the new generation.
* The best results were found where, for each generation, we keep around the best 1/4th of the total individuals. Keeping more than that worsen the result; keeping less than that makes the result improve faster in the first generations but stop improving sooner.
* Generating more offsprings gives better result, but of course it slower the computation.
* Anyway, after some generations the result stops improving, or improves very slowly. For example with N=50 we find the fitter individual with cost=103 at generation 55. Increasing the number of generations, we get to cost=102 at generation 187 (**More than 3 times the number of generations, for an improvement of only 1**), and then no improvement up to generation 300.

